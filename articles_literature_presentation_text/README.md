[Ссылка на презентацию](https://docs.google.com/presentation/d/1RZeNEXYrcnuPTD-LqarVNCz0Z1zmET0Uk2Z3OrIo8jI/edit?slide=id.g3aa0598f5ac_2_170#slide=id.g3aa0598f5ac_2_170)
\\
https://docs.google.com/presentation/d/1fcn-dKfaf1HT9fE5AK-U6aoBFv9VtUEEMhNfs-xKb-Q/edit?slide=id.g3ad74df734f_0_24#slide=id.g3ad74df734f_0_24 - статья по SRL
\\
Статьи
1) **Scaling Up RL: Unlocking Diverse Reasoning in LLMs via Prolonged Training:** [arXiv:2507.12507](https://arxiv.org/abs/2507.12507) (Масштабирование обучения с подкреплением на основе проверяемой обратной связи **(RLVF)** на триллионы токенов, что приводит к значительному улучшению способностей к рассуждению в различных областях без ущерба для языковых навыков.)
2) **rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking**: [arXiv:2501.04519](https://arxiv.org/abs/2501.04519) (**RSA** — рекурсивная само-агрегация множественных траекторий рассуждений для углубленного мышления **(не MCTS)**)
3) **Supervised Reinforcement Learning: From Expert Trajectories to Step-wise Reasoning:** 	[arXiv:2510.25992](https://arxiv.org/abs/2510.25992) (Гибридный метод, сочетающий обучение на экспертных траекториях (Supervised Learning) с пошаговым вознаграждением (Reinforcement Learning) для обучения сложным рассуждениям **(SRL)**)
4) **Does Math Reasoning Improve General LLM Capabilities? Understanding Transferability of LLM Reasoning:** [arXiv:2507.00432](https://arxiv.org/abs/2507.00432) (SFT не позволяет обобщать модель между областями математики, в отличии от RL, благодаря которому модель может решать задачи из других доменов)
5) **Recursive Self-Aggregation Unlocks Deep Thinking in Large Language Models**: [arXiv:2509.26626](https://arxiv.org/abs/2509.26626) **(RSA)**
6) **BLEU: a Method for Automatic Evaluation of MachineTranslation**: [статья](https://aclanthology.org/P02-1040.pdf)
7) [**Реализация sacrebleu**](https://github.com/mjpost/sacrebleu)
